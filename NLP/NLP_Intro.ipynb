{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FecqqeJW6XcL"
   },
   "outputs": [],
   "source": [
    "# This is an introduction to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ACaxBXZ-6cIX"
   },
   "source": [
    "**Preprocessing text**\n",
    "\n",
    "Cleaning and preparation are crucial for many tasks, and NLP is no exception. Text preprocessing is usually the first step you’ll take when faced with an NLP task.\n",
    "\n",
    "To preprocess the text regex and NLTK will do most of it for you! Common tasks include:\n",
    "\n",
    "Noise removal — stripping text of formatting (e.g., HTML tags).\n",
    "\n",
    "Tokenization — breaking text into individual words.\n",
    "\n",
    "Normalization — cleaning text data in any other way:\n",
    "\n",
    "    Stemming to chop off word prefixes and suffixes. “booing” and “booed” become “boo”, but “sing” may become “s” and “sung” would remain “sung.”\n",
    "    Lemmatization to bring words down to their root forms. For example, NLTK’s savvy lemmatizer knows “am” and “are” are related to “be.”\n",
    "    Other common tasks include lowercasing, stopwords removal, spelling correction, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "KXbwUKAX7Yux",
    "outputId": "ec12cf4c-027b-4562-ad59-12d877a8d1b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dmiranda/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "nO6IoljG7ICE",
    "outputId": "2fd9ac47-4f2c-4918-d2fa-b5b2830f913f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed text:\n",
      "['So', 'mani', 'squid', 'are', 'jump', 'out', 'of', 'suitcas', 'these', 'day', 'that', 'you', 'can', 'bare', 'go', 'anywher', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightli', 'pack', 'valis', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angri', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minut', 'of', 'arriv', 'she', 'hardli', 'even', 'notic']\n",
      "\n",
      "Lemmatized text:\n",
      "['So', 'many', 'squid', 'are', 'jumping', 'out', 'of', 'suitcase', 'these', 'day', 'that', 'you', 'can', 'barely', 'go', 'anywhere', 'without', 'seeing', 'one', 'burst', 'forth', 'from', 'a', 'tightly', 'packed', 'valise', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angry', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minute', 'of', 'arriving', 'She', 'hardly', 'even', 'noticed']\n"
     ]
    }
   ],
   "source": [
    "# regex for removing punctuation!\n",
    "import re\n",
    "# nltk preprocessing magic\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# grabbing a part of speech function:\n",
    "#from part_of_speech import get_part_of_speech\n",
    "\n",
    "text = \"So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise. I went to the dentist the other day, and sure enough I saw an angry one jump out of my dentist's bag within minutes of arriving. She hardly even noticed.\"\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text)\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(token) for token in tokenized]\n",
    "\n",
    "print(\"Stemmed text:\")\n",
    "print(stemmed)\n",
    "print(\"\\nLemmatized text:\")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P_pHLTaw_-4N"
   },
   "source": [
    "**Parsing**\n",
    "\n",
    "It may be helpful to know how the words relate to each other and the underlying syntax (grammar). Parsing is a stage of NLP concerned with segmenting text based on syntax.\n",
    "\n",
    "You probably do not want to be doing any parsing by hand and NLTK has a few tricks up its sleeve to help you out:\n",
    "\n",
    "Part-of-speech tagging (POS tagging) identifies parts of speech (verbs, nouns, adjectives, etc.). NLTK can do it faster (and maybe more accurately) than your grammar teacher.\n",
    "\n",
    "Named entity recognition (NER) helps identify the proper nouns (e.g., “Natalia” or “Berlin”) in a text. This can be a clue as to the topic of the text and NLTK captures many for you.\n",
    "\n",
    "Dependency grammar trees help you understand the relationship between the words in a sentence. It can be a tedious task for a human, so the Python library spaCy is at your service, even if it isn’t always perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    jumping                                                   \n",
      "  _____________________________________|__________________                                     \n",
      " |   |    |        |       |                              go                                  \n",
      " |   |    |        |       |     _________________________|____________                        \n",
      " |   |    |        |       |    |      |     |    |       |         without                   \n",
      " |   |    |        |       |    |      |     |    |       |            |                       \n",
      " |   |    |        |       |    |      |     |    |       |          seeing                   \n",
      " |   |    |        |       |    |      |     |    |       |            |                       \n",
      " |   |    |        |       |    |      |     |    |       |          burst                    \n",
      " |   |    |        |       |    |      |     |    |       |       _____|__________             \n",
      " |   |    |        |       |    |      |     |    |       |      |     |         from         \n",
      " |   |    |        |       |    |      |     |    |       |      |     |          |            \n",
      " |   |  squids    out      |    |      |     |    |       |      |     |        valise        \n",
      " |   |    |        |       |    |      |     |    |       |      |     |      ____|_______     \n",
      " |   |   many      of     days  |      |     |    |       |      |     |     |          packed\n",
      " |   |    |        |       |    |      |     |    |       |      |     |     |            |    \n",
      "are  .    So   suitcases these that   you   can barely anywhere one  forth   a         tightly\n",
      "\n",
      "                                      went                                               \n",
      "  _____________________________________|________________                                  \n",
      " |   |   |   |     |         |                         saw                               \n",
      " |   |   |   |     |         |          ________________|____                             \n",
      " |   |   |   |     |         |         |     |              jump                         \n",
      " |   |   |   |     |         |         |     |      _________|______________________      \n",
      " |   |   |   |     |         |         |     |     |    |    |         out          |    \n",
      " |   |   |   |     |         |         |     |     |    |    |          |           |     \n",
      " |   |   |   |     |         |         |     |     |    |    |          of        within \n",
      " |   |   |   |     |         |         |     |     |    |    |          |           |     \n",
      " |   |   |   |     to        |         |     |     |    |    |         bag       minutes \n",
      " |   |   |   |     |         |         |     |     |    |    |          |           |     \n",
      " |   |   |   |  dentist     day        |   enough  |    |    |       dentist        of   \n",
      " |   |   |   |     |      ___|____     |     |     |    |    |     _____|_____      |     \n",
      " I   ,  and  .    the   the     other  I    sure   an angry one   my          's arriving\n",
      "\n",
      "    noticed         \n",
      "  _____|__________   \n",
      "She  hardly even  . \n",
      "\n",
      "           is                        \n",
      "   ________|_____                     \n",
      "  |           pleasure               \n",
      "  |     _________|_______             \n",
      "  |    |                 in          \n",
      "  |    |                 |            \n",
      "  |    |               woods         \n",
      "  |    |          _______|______      \n",
      "There  a        teh          pathless\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "dependency_parser = spacy.load('en')\n",
    "\n",
    "text = \"So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise. I went to the dentist the other day, and sure enough I saw an angry one jump out of my dentist's bag within minutes of arriving. She hardly even noticed.\"\n",
    "\n",
    "parsed_text = dependency_parser(text)\n",
    "\n",
    "# Assign my_sentence a new value:\n",
    "my_sentence = \"There is a pleasure in teh pathless woods\"\n",
    "my_parsed_sentence = dependency_parser(my_sentence)\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        parsed_child_nodes = [to_nltk_tree(child) for child in node.children]\n",
    "        return Tree(node.orth_, parsed_child_nodes)\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "for sent in parsed_text.sents:\n",
    "    to_nltk_tree(sent.root).pretty_print()\n",
    "\n",
    "for sent in my_parsed_sentence.sents:\n",
    "    to_nltk_tree(sent.root).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JGWm50dpAzr_"
   },
   "source": [
    "**Language Models - Bag-of-Words Approach**\n",
    "\n",
    "How can we help a machine make sense of a bunch of word tokens? We can help computers make predictions about language by training a language model on a corpus (a bunch of example text).\n",
    "\n",
    "Language models are probabilistic computer models of language. We build and use these models to figure out the likelihood that a given sound, letter, word, or phrase will be used. Once a model has been trained, it can be tested out on new texts.\n",
    "\n",
    "One of the most common language models is the unigram model, a statistical language model commonly known as bag-of-words. As its name suggests, bag-of-words does not have much order to its chaos! What it does have is a tally count of each instance for each word. Consider the following text example:\n",
    "\n",
    "The squids jumped out of the suitcases\n",
    "Provided some initial preprocessing, bag-of-words would result in a mapping like:\n",
    "\n",
    "{\"the\": 2, \"squid\": 1, \"jump\": 1, \"out\": 1, \"of\": 1, \"suitcase\": 1}\n",
    "\n",
    "Now look at this sentence and mapping: “Why are your suitcases full of jumping squids?”\n",
    "\n",
    "{\"why\": 1, \"be\": 1, \"your\": 1, \"suitcase\": 1, \"full\": 1, \"of\": 1, \"jump\": 1, \"squid\": 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "5ibP3iYFA9kJ",
    "outputId": "bb4f56fc-9530-4a79-eace-5009b49290ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'humpty': 19, 'dumpty': 19, 'said': 17, 'alice': 16, 'name': 7, 'like': 7, 'im': 5, 'egg': 4, 'thought': 4, 'fall': 4, 'know': 4, 'king': 4, 'would': 4, 'dont': 4, 'come': 3, 'might': 3, 'one': 3, 'didnt': 3, 'must': 3, 'hand': 3, 'looking': 3, 'remark': 3, 'never': 3, 'last': 3, 'wall': 3, 'horse': 3, 'men': 3, 'almost': 3, 'mean': 3, 'shape': 3, 'good': 3, 'think': 3, 'another': 3, 'went': 3, 'however': 2, 'larger': 2, 'saw': 2, 'eye': 2, 'mouth': 2, 'cant': 2, 'written': 2, 'face': 2, 'time': 2, 'narrow': 2, 'quite': 2, 'could': 2, 'take': 2, 'long': 2, 'away': 2, 'looked': 2, 'gently': 2, 'explained': 2, 'added': 2, 'turn': 2, 'say': 2, 'conversation': 2, 'couldnt': 2, 'much': 2, 'interrupted': 2, 'course': 2, 'short': 2, 'there': 2, 'cried': 2, 'ask': 2, 'riddle': 2, 'thats': 2, 'behind': 2, 'book': 2, 'may': 2, 'ear': 2, 'little': 2, 'afraid': 2, 'old': 2, 'meant': 2, 'id': 2, 'got': 1, 'human': 1, 'within': 1, 'yard': 1, 'nose': 1, 'close': 1, 'clearly': 1, 'anybody': 1, 'else': 1, 'certain': 1, 'hundred': 1, 'easily': 1, 'enormous': 1, 'sitting': 1, 'leg': 1, 'crossed': 1, 'turk': 1, 'top': 1, 'high': 1, 'wallsuch': 1, 'wondered': 1, 'keep': 1, 'balanceand': 1, 'steadily': 1, 'fixed': 1, 'opposite': 1, 'direction': 1, 'least': 1, 'notice': 1, 'stuffed': 1, 'figure': 1, 'exactly': 1, 'aloud': 1, 'standing': 1, 'ready': 1, 'catch': 1, 'every': 1, 'moment': 1, 'expecting': 1, 'provoking': 1, 'silence': 1, 'spoke': 1, 'called': 1, 'eggvery': 1, 'sir': 1, 'pretty': 1, 'hoping': 1, 'sort': 1, 'compliment': 1, 'people': 1, 'usual': 1, 'sense': 1, 'baby': 1, 'wasnt': 1, 'anything': 1, 'fact': 1, 'evidently': 1, 'addressed': 1, 'treeso': 1, 'stood': 1, 'softly': 1, 'repeated': 1, 'sat': 1, 'great': 1, 'put': 1, 'place': 1, 'line': 1, 'poetry': 1, 'loud': 1, 'forgetting': 1, 'hear': 1, 'stand': 1, 'chattering': 1, 'first': 1, 'tell': 1, 'business': 1, 'stupid': 1, 'enough': 1, 'impatiently': 1, 'something': 1, 'asked': 1, 'doubtfully': 1, 'laugh': 1, 'amand': 1, 'handsome': 1, 'sit': 1, 'alone': 1, 'wishing': 1, 'begin': 1, 'argument': 1, 'nobody': 1, 'answer': 1, 'youd': 1, 'safer': 1, 'ground': 1, 'idea': 1, 'making': 1, 'simply': 1, 'natured': 1, 'anxiety': 1, 'queer': 1, 'creature': 1, 'tremendously': 1, 'easy': 1, 'growled': 1, 'ever': 1, 'offwhich': 1, 'chance': 1, 'ofbut': 1, 'pursed': 1, 'lip': 1, 'solemn': 1, 'grand': 1, 'hardly': 1, 'help': 1, 'laughing': 1, 'promised': 1, 'mewith': 1, 'mouthtoto': 1, 'send': 1, 'rather': 1, 'unwisely': 1, 'declare': 1, 'bad': 1, 'breaking': 1, 'sudden': 1, 'passion': 1, 'youve': 1, 'listening': 1, 'doorsand': 1, 'treesand': 1, 'chimneysor': 1, 'known': 1, 'havent': 1, 'indeed': 1, 'ah': 1, 'well': 1, 'write': 1, 'thing': 1, 'calmer': 1, 'tone': 1, 'call': 1, 'history': 1, 'england': 1, 'look': 1, 'spoken': 1, 'mayhap': 1, 'youll': 1, 'see': 1, 'show': 1, 'proud': 1, 'shake': 1, 'grinned': 1, 'leant': 1, 'forward': 1, 'nearly': 1, 'possible': 1, 'fell': 1, 'offered': 1, 'watched': 1, 'anxiously': 1, 'took': 1, 'smiled': 1, 'end': 1, 'meet': 1, 'happen': 1, 'head': 1, 'yes': 1, 'theyd': 1, 'pick': 1, 'minute': 1, 'going': 1, 'fast': 1, 'let': 1, 'go': 1, 'back': 1, 'remember': 1, 'politely': 1, 'case': 1, 'start': 1, 'fresh': 1, 'choose': 1, 'subject': 1, 'talk': 1, 'game': 1, 'here': 1, 'question': 1, 'made': 1, 'calculation': 1, 'seven': 1, 'year': 1, 'six': 1, 'month': 1, 'wrong': 1, 'exclaimed': 1, 'triumphantly': 1, 'word': 1, 'though': 1})\n"
     ]
    }
   ],
   "source": [
    "# importing regex and nltk\n",
    "import re, nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# importing Counter to get word counts for bag of words\n",
    "from collections import Counter\n",
    "# importing a passage from Through the Looking Glass\n",
    "from looking_glass import looking_glass_text\n",
    "# importing part-of-speech function for lemmatization\n",
    "#from part_of_speech import get_part_of_speech\n",
    "\n",
    "# Change text to another string:\n",
    "text = looking_glass_text\n",
    "#text = \"So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise. I went to the dentist the other day, and sure enough I saw an angry one jump out of my dentist's bag within minutes of arriving. She hardly even noticed.\"\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text).lower()\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "filtered = [word for word in tokenized if word not in stop_words]\n",
    "\n",
    "normalizer = WordNetLemmatizer()\n",
    "normalized = [normalizer.lemmatize(token) for token in filtered]\n",
    "# Comment out the print statement below\n",
    "#print(normalized)\n",
    "\n",
    "# Define bag_of_looking_glass_words & print:\n",
    "bag_of_looking_glass_words = Counter(normalized)\n",
    "print(bag_of_looking_glass_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AuZdZBrZB83r"
   },
   "source": [
    "Language Models - N-Grams and NLM\n",
    "\n",
    "For parsing entire phrases or conducting language prediction, you will want to use a model that pays attention to each word’s neighbors. Unlike bag-of-words, the n-gram model considers a sequence of some number (n) units and calculates the probability of each unit in a body of language given the preceding sequence of length n. Because of this, n-gram probabilities with larger n values can be impressive at language prediction.\n",
    "\n",
    "Take a look at our revised squid example: “The squids jumped out of the suitcases. The squids were furious.”\n",
    "\n",
    "A bigram model (where n is 2) might give us the following count frequencies:\n",
    "\n",
    "{('', 'the'): 2, ('the', 'squids'): 2, ('squids', 'jumped'): 1, ('jumped', 'out'): 1, ('out', 'of'): 1, ('of', 'the'): 1, ('the', 'suitcases'): 1, ('suitcases', ''): 1, ('squids', 'were'): 1, ('were', 'furious'): 1, ('furious', ''): 1}\n",
    "\n",
    "There are a couple problems with the n gram model:\n",
    "\n",
    "1. How can your language model make sense of the sentence “The cat fell asleep in the mailbox” if it’s never seen the word “mailbox” before? During training, your model will probably come across test words that it has never encountered before (this issue also pertains to bag of words). A tactic known as language smoothing can help adjust probabilities for unknown words, but it isn’t always ideal.\n",
    "\n",
    "2. For a model that more accurately predicts human language patterns, you want n (your sequence length) to be as large as possible. That way, you will have more natural sounding language, right? Well, as the sequence length grows, the number of examples of each sequence within your training corpus shrinks. With too few examples, you won’t have enough data to make many predictions.\n",
    "\n",
    "Enter neural language models (NLM)! Much recent work within NLP has involved developing and training neural networks to approximate the approach our human brains take towards language. This deep learning approach allows computers a much more adaptive tack to processing human language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "bU4GOqSECDFf",
    "outputId": "57f0728d-8527-4fe7-ed61-9955dc49a028"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Bigrams:\n",
      "[(('out', 'of'), 2), (('so', 'many'), 1), (('many', 'squids'), 1), (('squids', 'are'), 1), (('are', 'jumping'), 1), (('jumping', 'out'), 1), (('of', 'suitcases'), 1), (('suitcases', 'these'), 1), (('these', 'days'), 1), (('days', 'that'), 1)]\n",
      "\n",
      "Text Trigrams:\n",
      "[(('so', 'many', 'squids'), 1), (('many', 'squids', 'are'), 1), (('squids', 'are', 'jumping'), 1), (('are', 'jumping', 'out'), 1), (('jumping', 'out', 'of'), 1), (('out', 'of', 'suitcases'), 1), (('of', 'suitcases', 'these'), 1), (('suitcases', 'these', 'days'), 1), (('these', 'days', 'that'), 1), (('days', 'that', 'you'), 1)]\n",
      "\n",
      "Text n-grams:\n",
      "[(('so', 'many', 'squids', 'are'), 1), (('many', 'squids', 'are', 'jumping'), 1), (('squids', 'are', 'jumping', 'out'), 1), (('are', 'jumping', 'out', 'of'), 1), (('jumping', 'out', 'of', 'suitcases'), 1), (('out', 'of', 'suitcases', 'these'), 1), (('of', 'suitcases', 'these', 'days'), 1), (('suitcases', 'these', 'days', 'that'), 1), (('these', 'days', 'that', 'you'), 1), (('days', 'that', 'you', 'can'), 1)]\n"
     ]
    }
   ],
   "source": [
    "import nltk, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "# importing ngrams module from nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "text = \"So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise. I went to the dentist the other day, and sure enough I saw an angry one jump out of my dentist's bag within minutes of arriving. She hardly even noticed.\"\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text).lower()\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "text_bigrams = ngrams(tokenized, 2)\n",
    "text_bigrams_frequency = Counter(text_bigrams)\n",
    "\n",
    "text_trigrams = ngrams(tokenized, 3)\n",
    "text_trigrams_frequency = Counter(text_trigrams)\n",
    "\n",
    "text_ngrams = ngrams(tokenized, 4)\n",
    "text_ngrams_frequency = Counter(text_ngrams)\n",
    "\n",
    "print(\"Text Bigrams:\")\n",
    "print(text_bigrams_frequency.most_common(10))\n",
    "\n",
    "print(\"\\nText Trigrams:\")\n",
    "print(text_trigrams_frequency.most_common(10))\n",
    "\n",
    "print(\"\\nText n-grams:\")\n",
    "print(text_ngrams_frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ytjoQisACs-M"
   },
   "source": [
    "**Topic Models**\n",
    "\n",
    "We’ve touched on the idea of finding topics within a body of language. But what if the text is long and the topics aren’t obvious?\n",
    "\n",
    "Topic modeling is an area of NLP dedicated to uncovering latent, or hidden, topics within a body of language. For example, one Codecademy curriculum developer used topic modeling to discover patterns within Taylor Swift songs related to love and heartbreak over time.\n",
    "\n",
    "A common technique is to deprioritize the most common words and prioritize less frequently used terms as topics in a process known as term frequency-inverse document frequency (tf-idf). Say what?! This may sound counter-intuitive at first. Why would you want to give more priority to less-used words? Well, when you’re working with a lot of text, it makes a bit of sense if you don’t want your topics filled with words like “the” and “is.” The Python libraries gensim and sklearn have modules to handle tf-idf.\n",
    "\n",
    "Whether you use your plain bag of words (which will give you term frequency) or run it through tf-idf, the next step in your topic modeling journey is often latent Dirichlet allocation (LDA). LDA is a statistical model that takes your documents and determines which words keep popping up together in the same contexts (i.e., documents). We’ll use sklearn to tackle this for us.\n",
    "\n",
    "If you have any interest in visualizing your newly minted topics, word2vec is a great technique to have up your sleeve. word2vec can map out your topic model results spatially as vectors so that similarly used words are closer together. In the case of a language sample consisting of “The squids jumped out of the suitcases. The squids were furious. Why are your suitcases full of jumping squids?”, we might see that “suitcase”, “jump”, and “squid” were words used within similar contexts. This word-to-vector mapping is known as a word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "id": "DG1nOTf2C9rI",
    "outputId": "98c15871-03f7-4227-9bb8-ac3024da9a3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~ Topics found by bag of words LDA ~~~\n",
      "Topic #1: could would may leave\n",
      "Topic #2: could would find look\n",
      "Topic #3: could would may might\n",
      "Topic #4: would could find mr\n",
      "Topic #5: mccarthy drive sit laugh\n",
      "Topic #6: find mccarthy may case\n",
      "Topic #7: could thing hand father\n",
      "Topic #8: could leave take word\n",
      "Topic #9: would little face father\n",
      "Topic #10: father young little could\n",
      "\n",
      "\n",
      "~~~ Topics found by tf-idf LDA ~~~\n",
      "Topic #1: retire bye woman weapon\n",
      "Topic #2: title blue probable tree\n",
      "Topic #3: safe avoid investigation formidable\n",
      "Topic #4: raise order james boot\n",
      "Topic #5: hold death rattle silence\n",
      "Topic #6: tall weight nature paper\n",
      "Topic #7: consider wonderful miss former\n",
      "Topic #8: holmes say upon man\n",
      "Topic #9: care holmes white mccarthy\n",
      "Topic #10: sound innocence house brougham\n"
     ]
    }
   ],
   "source": [
    "import nltk, re\n",
    "from sherlock_holmes import bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3\n",
    "from preprocessing import preprocess_text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# preparing the text\n",
    "corpus = [bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3]\n",
    "preprocessed_corpus = [preprocess_text(chapter) for chapter in corpus]\n",
    "\n",
    "# Update stop_list:\n",
    "stop_list = [\"say\", \"shall\", \"holmes\", \"see\", \"man\", \"upon\", \"know\", \"quite\", \"one\", \"come\", \"go\"]\n",
    "# filtering topics for stop words\n",
    "def filter_out_stop_words(corpus):\n",
    "    no_stops_corpus = []\n",
    "    for chapter in corpus:\n",
    "        no_stops_chapter = \" \".join([word for word in chapter.split(\" \") if word not in stop_list])\n",
    "        no_stops_corpus.append(no_stops_chapter)\n",
    "    return no_stops_corpus\n",
    "filtered_for_stops = filter_out_stop_words(preprocessed_corpus)\n",
    "\n",
    "# creating the bag of words model\n",
    "bag_of_words_creator = CountVectorizer()\n",
    "bag_of_words = bag_of_words_creator.fit_transform(filtered_for_stops)\n",
    "\n",
    "# creating the tf-idf model\n",
    "tfidf_creator = TfidfVectorizer(min_df = 0.2)\n",
    "tfidf = tfidf_creator.fit_transform(preprocessed_corpus)\n",
    "\n",
    "# creating the bag of words LDA model\n",
    "lda_bag_of_words_creator = LatentDirichletAllocation(learning_method='online', n_components=10)\n",
    "lda_bag_of_words = lda_bag_of_words_creator.fit_transform(bag_of_words)\n",
    "\n",
    "# creating the tf-idf LDA model\n",
    "lda_tfidf_creator = LatentDirichletAllocation(learning_method='online', n_components=10)\n",
    "lda_tfidf = lda_tfidf_creator.fit_transform(tfidf)\n",
    "\n",
    "print(\"~~~ Topics found by bag of words LDA ~~~\")\n",
    "for topic_id, topic in enumerate(lda_bag_of_words_creator.components_):\n",
    "    message = \"Topic #{}: \".format(topic_id + 1)\n",
    "    message += \" \".join([bag_of_words_creator.get_feature_names()[i] for i in topic.argsort()[:-5 :-1]])\n",
    "    print(message)\n",
    "\n",
    "print(\"\\n\\n~~~ Topics found by tf-idf LDA ~~~\")\n",
    "for topic_id, topic in enumerate(lda_tfidf_creator.components_):\n",
    "    message = \"Topic #{}: \".format(topic_id + 1)\n",
    "    message += \" \".join([tfidf_creator.get_feature_names()[i] for i in topic.argsort()[:-5 :-1]])\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kf95yTSPT060"
   },
   "source": [
    "**Text Similarity**\n",
    "\n",
    "Most of us have a good autocorrect story. Our phone’s messenger quietly swaps one letter for another as we type and suddenly the meaning of our message has changed (to our horror or pleasure). However, addressing text similarity — including spelling correction — is a major challenge within natural language processing.\n",
    "\n",
    "Addressing word similarity and misspelling for spellcheck or autocorrect often involves considering the Levenshtein distance or minimal edit distance between two words. The distance is calculated through the minimum number of insertions, deletions, and substitutions that would need to occur for one word to become another. For example, turning “bees” into “beans” would require one substitution (“a” for “e”) and one insertion (“n”), so the Levenshtein distance would be two.\n",
    "\n",
    "Phonetic similarity is also a major challenge within speech recognition. English-speaking humans can easily tell from context whether someone said “euthanasia” or “youth in Asia,” but it’s a far more challenging task for a machine! More advanced autocorrect and spelling correction technology additionally considers key distance on a keyboard and phonetic similarity (how much two words or phrases sound the same).\n",
    "\n",
    "It’s also helpful to find out if texts are the same to guard against plagiarism, which we can identify through lexical similarity (the degree to which texts use the same vocabulary and phrases). Meanwhile, semantic similarity (the degree to which documents contain similar meaning or topics) is useful when you want to find (or recommend) an article or book similar to one you recently finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "2MSO8hzAT4fg",
    "outputId": "9a3c8c91-0d0f-464e-bd6f-022291db0b37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Levenshtein distance from 'fart' to 'target' is 3!\n",
      "The Levenshtein distance from 'code' to 'cars' is 3!\n",
      "The Levenshtein distance from 'chunk' to 'check' is 2!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# NLTK has a built-in function\n",
    "# to check Levenshtein distance:\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "def print_levenshtein(string1, string2):\n",
    "    print(\"The Levenshtein distance from '{0}' to '{1}' is {2}!\".format(string1, string2, edit_distance(string1, string2)))\n",
    "\n",
    "# Check the distance between\n",
    "# any two words here!\n",
    "print_levenshtein(\"fart\", \"target\")\n",
    "\n",
    "# Assign passing strings here:\n",
    "three_away_from_code = \"cars\"\n",
    "\n",
    "two_away_from_chunk = \"check\"\n",
    "\n",
    "print_levenshtein(\"code\", three_away_from_code)\n",
    "print_levenshtein(\"chunk\", two_away_from_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lPlfJKZAT-QD"
   },
   "source": [
    "**Language Prediction & Text Generation**\n",
    "\n",
    "How does your favorite search engine complete your search queries? How does your phone’s keyboard know what you want to type next? Language prediction is an application of NLP concerned with predicting text given preceding text. Autosuggest, autocomplete, and suggested replies are common forms of language prediction.\n",
    "\n",
    "Your first step to language prediction is picking a language model. Bag of words alone is generally not a great model for language prediction; no matter what the preceding word was, you will just get one of the most commonly used words from your training corpus.\n",
    "\n",
    "If you go the n-gram route, you will most likely rely on Markov chains to predict the statistical likelihood of each following word (or character) based on the training corpus. Markov chains are memory-less and make statistical predictions based entirely on the current n-gram on hand.\n",
    "\n",
    "For example, let’s take a sentence beginning, “I ate so many grilled cheese”. Using a trigram model (where n is 3), a Markov chain would predict the following word as “sandwiches” based on the number of times the sequence “grilled cheese sandwiches” has appeared in the training data out of all the times “grilled cheese” has appeared in the training data.\n",
    "\n",
    "A more advanced approach, using a neural language model, is the Long Short Term Memory (LSTM) model. LSTM uses deep learning with a network of artificial “cells” that manage memory, making them better suited for text prediction than traditional neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kT9f0-ieUHS1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i don t know what he s in this whole back nod now the whole back to cross between a costly mistake if they sleepin on you are heavy there goes back nod now the lab again yo this bitch i can feel it like a h a h a\n"
     ]
    }
   ],
   "source": [
    "import nltk, re, random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict, deque\n",
    "from document1 import training_doc1\n",
    "from document2 import training_doc2\n",
    "from document3 import training_doc3\n",
    "\n",
    "class MarkovChain:\n",
    "    def __init__(self):\n",
    "        self.lookup_dict = defaultdict(list)\n",
    "        self._seeded = False\n",
    "        self.__seed_me()\n",
    "\n",
    "    def __seed_me(self, rand_seed=None):\n",
    "        if self._seeded is not True:\n",
    "            try:\n",
    "                if rand_seed is not None:\n",
    "                    random.seed(rand_seed)\n",
    "                else:\n",
    "                    random.seed()\n",
    "                self._seeded = True\n",
    "            except NotImplementedError:\n",
    "                self._seeded = False\n",
    "    \n",
    "    def add_document(self, str):\n",
    "        preprocessed_list = self._preprocess(str)\n",
    "        pairs = self.__generate_tuple_keys(preprocessed_list)\n",
    "        for pair in pairs:\n",
    "            self.lookup_dict[pair[0]].append(pair[1])\n",
    "  \n",
    "    def _preprocess(self, str):\n",
    "        cleaned = re.sub(r'\\W+', ' ', str).lower()\n",
    "        tokenized = word_tokenize(cleaned)\n",
    "        return tokenized\n",
    "\n",
    "    def __generate_tuple_keys(self, data):\n",
    "        if len(data) < 1:\n",
    "            return\n",
    "\n",
    "        for i in range(len(data) - 1):\n",
    "            yield [ data[i], data[i + 1] ]\n",
    "      \n",
    "    def generate_text(self, max_length=50):\n",
    "        context = deque()\n",
    "        output = []\n",
    "        if len(self.lookup_dict) > 0:\n",
    "            self.__seed_me(rand_seed=len(self.lookup_dict))\n",
    "            chain_head = [list(self.lookup_dict)[0]]\n",
    "            context.extend(chain_head)\n",
    "      \n",
    "            while len(output) < (max_length - 1):\n",
    "                next_choices = self.lookup_dict[context[-1]]\n",
    "                if len(next_choices) > 0:\n",
    "                    next_word = random.choice(next_choices)\n",
    "                    context.append(next_word)\n",
    "                    output.append(context.popleft())\n",
    "                else:\n",
    "                    break\n",
    "            output.extend(list(context))\n",
    "        return \" \".join(output)\n",
    "\n",
    "my_markov = MarkovChain()\n",
    "my_markov.add_document(training_doc1)\n",
    "my_markov.add_document(training_doc2)\n",
    "my_markov.add_document(training_doc3)\n",
    "generated_text = my_markov.generate_text()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nn-KWFJfUN3f"
   },
   "source": [
    "**Advanced NLP Topics**\n",
    "\n",
    "Believe it or not, you’ve just scratched the surface of natural language processing. There are a slew of advanced topics and applications of NLP, many of which rely on deep learning and neural networks.\n",
    "\n",
    "    Naive Bayes classifiers are supervised machine learning algorithms that leverage a probabilistic theorem to make predictions and classifications. They are widely used for sentiment analysis (determining whether a given block of language expresses negative or positive feelings) and spam filtering.\n",
    "\n",
    "    We’ve made enormous gains in machine translation, but even the most advanced translation software using neural networks and LSTM still has far to go in accurately translating between languages.\n",
    "\n",
    "    Some of the most life-altering applications of NLP are focused on improving language accessibility for people with disabilities. Text-to-speech functionality and speech recognition have improved rapidly thanks to neural language models, making digital spaces far more accessible places.\n",
    "\n",
    "    NLP can also be used to detect bias in writing and speech. Feel like a political candidate, book, or news source is biased but can’t put your finger on exactly how? Natural language processing can help you identify the language at issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reviews import counter, training_counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Add your review:\n",
    "review = \"great, learned lots of new things\"\n",
    "review_counts = counter.transform([review])\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "training_labels = [0] * 1000 + [1] * 1000\n",
    "\n",
    "classifier.fit(training_counts, training_labels)\n",
    "  \n",
    "neg = (classifier.predict_proba(review_counts)[0][0] * 100).round()\n",
    "pos = (classifier.predict_proba(review_counts)[0][1] * 100).round()\n",
    "\n",
    "if pos > 50:\n",
    "    print(\"Thank you for your positive review!\")\n",
    "elif neg > 50:\n",
    "    print(\"We're sorry this hasn't been the best possible lesson for you! We're always looking to improve.\")\n",
    "else:\n",
    "    print(\"Naive Bayes cannot determine if this is negative or positive. Thank you or we're sorry?\")\n",
    "\n",
    "print(\"\\nAccording to our trained Naive Bayes classifier, the probability that your review was negative was {0}% and the probability it was positive was {1}%.\".format(neg, pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xhb5MDefV8SK"
   },
   "source": [
    "**Challenges and Considerations**\n",
    "\n",
    "As you’ve seen, there are a vast array of applications for NLP. However, as they say, “with great language processing comes great responsibility” (or something along those lines). When working with NLP, we have several important considerations to take into account:\n",
    "\n",
    "    Different NLP tasks may be more or less difficult in different languages. Because so many NLP tools are built by and for English speakers, these tools may lag behind in processing other languages. The tools may also be programmed with cultural and linguistic biases specific to English speakers.\n",
    "    What if your Amazon Alexa could only understand wealthy men from coastal areas of the United States? English itself is not a homogeneous body. English varies by person, by dialect, and by many sociolinguistic factors. When we build and train NLP tools, are we only building them for one type of English speaker?\n",
    "    You can have the best intentions and still inadvertently program a bigoted tool. While NLP can limit bias, it can also propagate bias. As an NLP developer, it’s important to consider biases, both within your code and within the training corpus. A machine will learn the same biases you teach it, whether intentionally or unintentionally.\n",
    "    As you become someone who builds tools with natural language processing, it’s vital to take into account your users’ privacy. There are many powerful NLP tools that come head-to-head with privacy concerns. Who is collecting your data? How much data is being collected and what do those companies plan to do with your data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reviews import counter, training_counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Add your review:\n",
    "review = \"It was lit!\"\n",
    "review_counts = counter.transform([review])\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "training_labels = [0] * 1000 + [1] * 1000\n",
    "\n",
    "classifier.fit(training_counts, training_labels)\n",
    "\n",
    "neg = (classifier.predict_proba(review_counts)[0][0] * 100).round()\n",
    "pos = (classifier.predict_proba(review_counts)[0][1] * 100).round()\n",
    "\n",
    "if pos > 50:\n",
    "    print(\"Naive Bayes classifies this as positive.\")\n",
    "elif neg > 50:\n",
    "    print(\"Naive Bayes classifies this as negative.\")\n",
    "else:\n",
    "    print(\"Naive Bayes cannot determine if this is negative or positive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-LcBm7DkWLRj"
   },
   "source": [
    "**NLP Review**\n",
    "\n",
    "Check out how much you’ve learned about natural language processing!\n",
    "\n",
    "    Natural language processing combines computer science, linguistics, and artificial intelligence to enable computers to process human languages.\n",
    "    NLTK is a Python library used for NLP.\n",
    "    Text preprocessing is a stage of NLP focused on cleaning and preparing text for other NLP tasks.\n",
    "    Parsing is a stage of NLP concerned with breaking up text based on syntax.\n",
    "    Language models are probabilistic machine models of language use for NLP comprehension tasks. Common models include bag-of-words, n-gram models, and neural language modeling.\n",
    "    Topic modeling is the NLP process by which hidden topics are identified given a body of text.\n",
    "    Text similarity is a facet of NLP concerned with semblance between instances of language.\n",
    "    Language prediction is an application of NLP concerned with predicting language given preceding language.\n",
    "    There are many social and ethical considerations to take into account when designing NLP tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cKxgzA4oWM2d",
    "outputId": "3edfe613-628f-4c1a-f530-e4387c35ca56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Levenshtein distance:\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "# an arbitrary plagiarism classifier:\n",
    "def is_plagiarized(text1, text2):\n",
    "    n = 7\n",
    "    if edit_distance(text1.lower(), text2.lower()) > ((len(text1) + len(text2)) / n):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "doc1 = \"is this plagiarized\"\n",
    "doc2 = \"maybe it's plagiarized\"\n",
    "\n",
    "print(is_plagiarized(doc1, doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP-Intro.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
